{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# import torchvision\n",
    "# from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngpu = 1\n",
    "# Decide si queremos correr en gpu o cpu\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[',', 'de', 'que', 'el', 'la', 'a']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ix_to_w = open('word_labels_freeling.txt', 'r', encoding='utf-8').read().splitlines()\n",
    "ix_to_w[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7612"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_to_ix = {w: ix for ix, w in enumerate(ix_to_w)}\n",
    "vocab_size = len(w_to_ix)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, ngpu, D_in, D_emb, D_lstm, D_out):\n",
    "        super(Model, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.embedding = nn.Embedding(num_embeddings=D_in, embedding_dim=D_emb)#, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(input_size=D_emb, hidden_size=D_lstm) #, bias=True)#, batch_first=True)\n",
    "        self.linear = nn.Linear(in_features=D_lstm, out_features=D_out) #, bias=True)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        T = len(sentence)\n",
    "\n",
    "        embeddings = self.embedding(sentence).view(T, 1, -1)\n",
    "\n",
    "        lstm_out, (ht, ct) = self.lstm(embeddings)\n",
    "        lstm_out = lstm_out.view(T, -1)\n",
    "\n",
    "        preact_out = self.linear(lstm_out).view(T, -1)\n",
    "\n",
    "        return F.log_softmax(preact_out, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensión de entrada (one-hot), tamaño del vocabulario\n",
    "D_in = vocab_size\n",
    "\n",
    "# Dimensión de la capa de embedding\n",
    "D_emb = 64 # 32\n",
    "\n",
    "# Dimensión de la capa lstm\n",
    "D_lstm = 32 # 16\n",
    "\n",
    "# Dimensión de la capa de salida\n",
    "D_out = D_in\n",
    "\n",
    "# Modelo por cargar\n",
    "model_state = 'modelsaves/model-freeling-emb_64-lstm_32-seed_72-epochs_550'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (embedding): Embedding(7612, 64)\n",
       "  (lstm): LSTM(64, 32)\n",
       "  (linear): Linear(in_features=32, out_features=7612, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(ngpu, D_in, D_emb, D_lstm, D_out)\n",
    "model.load_state_dict(torch.load(model_state))\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS = '<BOS>'\n",
    "EOS = '<EOS>'\n",
    "UNK = '<UNK>'\n",
    "\n",
    "ixBOS = w_to_ix[BOS]\n",
    "ixEOS = w_to_ix[EOS]\n",
    "ixUNK = w_to_ix[UNK]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sents = [\n",
    "    'andrés_manuel_lópez_obrador',\n",
    "    'lópez_obrador',\n",
    "    'ricardo_anaya',\n",
    "    'anaya',\n",
    "    'josé_antonio_meade',\n",
    "    'meade',\n",
    "    'el candidato'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[7609, 64],\n",
       " [7609, 42],\n",
       " [7609, 51],\n",
       " [7609, 165],\n",
       " [7609, 46],\n",
       " [7609, 63],\n",
       " [7609, 3, 25]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sents_ix = [\n",
    "    [ixBOS]+[w_to_ix[w] for w in sent.split()] \n",
    "    for sent in test_sents\n",
    "]\n",
    "test_sents_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_pytorch_tensor(list_of_lists):\n",
    "    return [\n",
    "        torch.LongTensor(l).to(device)\n",
    "        for l in list_of_lists\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([7609,   64], device='cuda:0'),\n",
       " tensor([7609,   42], device='cuda:0'),\n",
       " tensor([7609,   51], device='cuda:0'),\n",
       " tensor([7609,  165], device='cuda:0'),\n",
       " tensor([7609,   46], device='cuda:0'),\n",
       " tensor([7609,   63], device='cuda:0'),\n",
       " tensor([7609,    3,   25], device='cuda:0')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = to_pytorch_tensor(test_sents_ix)\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_ix_sentence(sentence, end='\\n'):\n",
    "    print(' '.join(ix_to_w[ix] for ix in sentence.data), end=end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_sent(sentence, limit = 47, choose_max=True):\n",
    "    if len(sentence) == 0:\n",
    "        sentence = torch.LongTensor([ixBOS]).to(device)\n",
    "    torchEOS = torch.LongTensor([ixEOS]).to(device)\n",
    "    i = 0\n",
    "    prediccion = sentence[-1:]\n",
    "    while not torch.eq(prediccion, torchEOS):\n",
    "        out = model(sentence)\n",
    "        if choose_max:\n",
    "            prediccion = torch.argmax(out[-1:], dim=1)\n",
    "        else:\n",
    "            prediccion = torch.multinomial(torch.exp(out[-1]), 1)\n",
    "#         print_ix_sentence(prediccion, end=' ') # Imprimo el siguiente caracter\n",
    "        sentence = torch.cat((sentence, prediccion), dim=0) # Genero siguiente cadena\n",
    "        i+=1\n",
    "        if i>limit:\n",
    "            break\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================\n",
      "- Probando enunciado:\n",
      "<BOS> andrés_manuel_lópez_obrador\n",
      "\n",
      "- Enunciado generado:\n",
      "<BOS> andrés_manuel_lópez_obrador comentó que se llamó a el productor , como es su momento fundamental . <EOS>\n",
      "\n",
      "\n",
      "===================================================\n",
      "- Probando enunciado:\n",
      "<BOS> lópez_obrador\n",
      "\n",
      "- Enunciado generado:\n",
      "<BOS> lópez_obrador dijo que ya no se informó menos gobernador más colaboradores que en 30 legales para nuestra ayotzinapa . <EOS>\n",
      "\n",
      "\n",
      "===================================================\n",
      "- Probando enunciado:\n",
      "<BOS> ricardo_anaya\n",
      "\n",
      "- Enunciado generado:\n",
      "<BOS> ricardo_anaya lázaro_cárdenas estuvo acompañado en candidata de campaña saludó el encuentro con <UNK> , alto vive una comunicación : damián_zepeda , en nada un grupo entre todos , se va a pagar votos . <EOS>\n",
      "\n",
      "\n",
      "===================================================\n",
      "- Probando enunciado:\n",
      "<BOS> anaya\n",
      "\n",
      "- Enunciado generado:\n",
      "<BOS> anaya no está muy duarte . <EOS>\n",
      "\n",
      "\n",
      "===================================================\n",
      "- Probando enunciado:\n",
      "<BOS> josé_antonio_meade\n",
      "\n",
      "- Enunciado generado:\n",
      "<BOS> josé_antonio_meade congregó a meter con <UNK> . <EOS>\n",
      "\n",
      "\n",
      "===================================================\n",
      "- Probando enunciado:\n",
      "<BOS> meade\n",
      "\n",
      "- Enunciado generado:\n",
      "<BOS> meade se comprometió a que me están haciendo aranceles en un momento diagnóstico de apoyo . <EOS>\n",
      "\n",
      "\n",
      "===================================================\n",
      "- Probando enunciado:\n",
      "<BOS> el candidato\n",
      "\n",
      "- Enunciado generado:\n",
      "<BOS> el candidato de la coalición_por_méxico a el frente en invitó de un equipo especiales de niñas , guerrero . <EOS>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for sentence in X_test[:10]:\n",
    "        print('===================================================')\n",
    "        print('- Probando enunciado:')\n",
    "        print_ix_sentence(sentence)\n",
    "        print()\n",
    "        \n",
    "        print('- Enunciado generado:')\n",
    "        gen_sentence = generar_sent(sentence, limit=60, choose_max=False)\n",
    "        print_ix_sentence(gen_sentence)\n",
    "        print('\\n')\n",
    "\n",
    "#         out = model(sentence)\n",
    "#         prediccion = torch.argmax(out[-1:], dim=1)\n",
    "#         print('- Texto generado:')\n",
    "#         print_ix_sentence(prediccion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('aypit-2020-2': conda)",
   "language": "python",
   "name": "python37764bitaypit20202conda2eb9642ad5e346f38de29b14c14576bc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
