{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# import torchvision\n",
    "# from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngpu = 1\n",
    "# Decide si queremos correr en gpu o cpu\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[',', 'de', 'que', 'el', 'la', 'a']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ix_to_w = open('word_labels.txt', 'r', encoding='utf-8').read().splitlines()\n",
    "ix_to_w[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7796"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_to_ix = {w: ix for ix, w in enumerate(ix_to_w)}\n",
    "vocab_size = len(w_to_ix)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, ngpu, D_in, D_emb, D_lstm, D_out):\n",
    "        super(Model, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.embedding = nn.Embedding(num_embeddings=D_in, embedding_dim=D_emb)#, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(input_size=D_emb, hidden_size=D_lstm) #, bias=True)#, batch_first=True)\n",
    "        self.linear = nn.Linear(in_features=D_lstm, out_features=D_out) #, bias=True)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        T = len(sentence)\n",
    "\n",
    "        embeddings = self.embedding(sentence).view(T, 1, -1)\n",
    "\n",
    "        lstm_out, (ht, ct) = self.lstm(embeddings)\n",
    "        lstm_out = lstm_out.view(T, -1)\n",
    "\n",
    "        preact_out = self.linear(lstm_out).view(T, -1)\n",
    "\n",
    "        return F.log_softmax(preact_out, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensión de entrada (one-hot), tamaño del vocabulario\n",
    "D_in = vocab_size\n",
    "\n",
    "# Dimensión de la capa de embedding\n",
    "D_emb = 32 # 32\n",
    "\n",
    "# Dimensión de la capa lstm\n",
    "D_lstm = 16 # 16\n",
    "\n",
    "# Dimensión de la capa de salida\n",
    "D_out = D_in\n",
    "\n",
    "# Épocas de entrenamiento\n",
    "num_epochs = 300\n",
    "\n",
    "# Betas para Adam\n",
    "beta1 = 0.0001\n",
    "beta2 = 0.99\n",
    "\n",
    "# Learning rate\n",
    "lr =  0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (embedding): Embedding(7796, 32)\n",
       "  (lstm): LSTM(32, 16)\n",
       "  (linear): Linear(in_features=16, out_features=7796, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(ngpu, D_in, D_emb, D_lstm, D_out)\n",
    "model.load_state_dict(torch.load('model-seed42069'))\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS = '<BOS>'\n",
    "EOS = '<EOS>'\n",
    "UNK = '<UNK>'\n",
    "\n",
    "ixBOS = w_to_ix[BOS]\n",
    "ixEOS = w_to_ix[EOS]\n",
    "ixUNK = w_to_ix[UNK]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sents = [\n",
    "    'andrés_manuel_lópez_obrador',\n",
    "    'lópez_obrador',\n",
    "    'ricardo_anaya',\n",
    "    'anaya',\n",
    "    'josé_antonio_meade',\n",
    "    'meade',\n",
    "    'el candidato'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[7793, 64],\n",
       " [7793, 43],\n",
       " [7793, 51],\n",
       " [7793, 170],\n",
       " [7793, 44],\n",
       " [7793, 59],\n",
       " [7793, 3, 25]]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sents_ix = [\n",
    "    [ixBOS]+[w_to_ix[w] for w in sent.split()] \n",
    "    for sent in test_sents\n",
    "]\n",
    "test_sents_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_pytorch_tensor(list_of_lists):\n",
    "    return [\n",
    "        torch.LongTensor(l).to(device)\n",
    "        for l in list_of_lists\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([7793,   64], device='cuda:0'),\n",
       " tensor([7793,   43], device='cuda:0'),\n",
       " tensor([7793,   51], device='cuda:0'),\n",
       " tensor([7793,  170], device='cuda:0'),\n",
       " tensor([7793,   44], device='cuda:0'),\n",
       " tensor([7793,   59], device='cuda:0'),\n",
       " tensor([7793,    3,   25], device='cuda:0')]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = to_pytorch_tensor(test_sents_ix)\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_ix_sentence(sentence, end='\\n'):\n",
    "    print(' '.join(ix_to_w[ix] for ix in sentence.data), end=end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_sent(sentence, limit = 35, choose_max=True):\n",
    "    if len(sentence) == 0:\n",
    "        sentence = torch.LongTensor([ixBOS]).to(device)\n",
    "    torchEOS = torch.LongTensor([ixEOS]).to(device)\n",
    "    i = 0\n",
    "    prediccion = sentence[-1:]\n",
    "    while not torch.eq(prediccion, torchEOS):\n",
    "        out = model(sentence)\n",
    "        if choose_max:\n",
    "            prediccion = torch.argmax(out[-1:], dim=1)\n",
    "        else:\n",
    "            prediccion = torch.multinomial(torch.exp(out[-1]), 1)\n",
    "#         print_ix_sentence(prediccion, end=' ') # Imprimo el siguiente caracter\n",
    "        sentence = torch.cat((sentence, prediccion), dim=0) # Genero siguiente cadena\n",
    "        i+=1\n",
    "        if i>limit:\n",
    "            break\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================\n",
      "- Probando enunciado:\n",
      "<BOS> andrés_manuel_lópez_obrador\n",
      "\n",
      "- Enunciado generado:\n",
      "<BOS> andrés_manuel_lópez_obrador expresó que en los últimos días , porque no se <UNK> , a el país , porque no se <UNK> , a el país , porque no se <UNK> , a el país , porque no se <UNK> , a el país , porque no se <UNK> , a el país , porque no se <UNK> , a el país ,\n",
      "\n",
      "\n",
      "===================================================\n",
      "- Probando enunciado:\n",
      "<BOS> lópez_obrador\n",
      "\n",
      "- Enunciado generado:\n",
      "<BOS> lópez_obrador expresó que en la <UNK> de el país , no se <UNK> , a el país , porque se <UNK> a el país , porque no se <UNK> , a el país , porque no se <UNK> , a el país , porque no se <UNK> , a el país , porque no se <UNK> , a el país , porque\n",
      "\n",
      "\n",
      "===================================================\n",
      "- Probando enunciado:\n",
      "<BOS> ricardo_anaya\n",
      "\n",
      "- Enunciado generado:\n",
      "<BOS> ricardo_anaya estuvo en <UNK> , josé_antonio_meade , dijo , josé_antonio_meade , dijo . <EOS>\n",
      "\n",
      "\n",
      "===================================================\n",
      "- Probando enunciado:\n",
      "<BOS> anaya\n",
      "\n",
      "- Enunciado generado:\n",
      "<BOS> anaya no se <UNK> , a el país , porque no se <UNK> , a el país , porque no se <UNK> , a el país , porque no se <UNK> , a el país , porque no se <UNK> , a el país , porque no se <UNK> , a el país , porque no se <UNK> , a el país\n",
      "\n",
      "\n",
      "===================================================\n",
      "- Probando enunciado:\n",
      "<BOS> josé_antonio_meade\n",
      "\n",
      "- Enunciado generado:\n",
      "<BOS> josé_antonio_meade anunció que a el candidato de la coalición todos_por_méxico a la presidencia_de_la_república , josé_antonio_meade , dijo . <EOS>\n",
      "\n",
      "\n",
      "===================================================\n",
      "- Probando enunciado:\n",
      "<BOS> meade\n",
      "\n",
      "- Enunciado generado:\n",
      "<BOS> meade dijo que a la <UNK> , porque no se <UNK> , a el país , porque no se <UNK> , a el país , porque no se <UNK> , a el país , porque no se <UNK> , a el país , porque no se <UNK> , a el país , porque no se <UNK> , a el país , porque\n",
      "\n",
      "\n",
      "===================================================\n",
      "- Probando enunciado:\n",
      "<BOS> el candidato\n",
      "\n",
      "- Enunciado generado:\n",
      "<BOS> el candidato de la coalición todos_por_méxico a la presidencia_de_la_república , josé_antonio_meade , dijo . <EOS>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for sentence in X_test[:10]:\n",
    "        print('===================================================')\n",
    "        print('- Probando enunciado:')\n",
    "        print_ix_sentence(sentence)\n",
    "        print()\n",
    "        \n",
    "        print('- Enunciado generado:')\n",
    "        gen_sentence = generar_sent(sentence, limit=60)\n",
    "        print_ix_sentence(gen_sentence)\n",
    "        print('\\n')\n",
    "\n",
    "#         out = model(sentence)\n",
    "#         prediccion = torch.argmax(out[-1:], dim=1)\n",
    "#         print('- Texto generado:')\n",
    "#         print_ix_sentence(prediccion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000, device='cuda:0')"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(out[-1]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([16], device='cuda:0')"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.multinomial(torch.exp(out[-1]), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('aypit-2020-2': conda)",
   "language": "python",
   "name": "python37764bitaypit20202conda2eb9642ad5e346f38de29b14c14576bc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
