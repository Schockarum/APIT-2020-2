{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngpu = 1\n",
    "# Decide si queremos correr en gpu o cpu\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[',', 'de', 'que', 'la', 'a', 'el']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ix_to_w = open('word_labels_corenlp.txt', 'r', encoding='utf-8').read().splitlines()\n",
    "ix_to_w[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7474"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_to_ix = {w: ix for ix, w in enumerate(ix_to_w)}\n",
    "vocab_size = len(w_to_ix)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, ngpu, D_in, D_emb, D_lstm, D_out):\n",
    "        super(Model, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.embedding = nn.Embedding(num_embeddings=D_in, embedding_dim=D_emb)#, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(input_size=D_emb, hidden_size=D_lstm) #, bias=True)#, batch_first=True)\n",
    "        self.linear = nn.Linear(in_features=D_lstm, out_features=D_out) #, bias=True)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        T = len(sentence)\n",
    "\n",
    "        embeddings = self.embedding(sentence).view(T, 1, -1)\n",
    "\n",
    "        lstm_out, (ht, ct) = self.lstm(embeddings)\n",
    "        lstm_out = lstm_out.view(T, -1)\n",
    "\n",
    "        preact_out = self.linear(lstm_out).view(T, -1)\n",
    "\n",
    "        return F.log_softmax(preact_out, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensión de entrada (one-hot), tamaño del vocabulario\n",
    "D_in = vocab_size\n",
    "\n",
    "# Dimensión de la capa de embedding\n",
    "D_emb = 64 # 32\n",
    "\n",
    "# Dimensión de la capa lstm\n",
    "D_lstm = 32 # 16\n",
    "\n",
    "# Dimensión de la capa de salida\n",
    "D_out = D_in\n",
    "\n",
    "# Modelo por cargar\n",
    "model_state = 'modelsaves/modelv2-corenlp-emb_64-lstm_32-seed_42069-epochs_299-best-val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (embedding): Embedding(7474, 64)\n",
       "  (lstm): LSTM(64, 32)\n",
       "  (linear): Linear(in_features=32, out_features=7474, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(ngpu, D_in, D_emb, D_lstm, D_out)\n",
    "model.load_state_dict(torch.load(model_state))\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS = '<BOS>'\n",
    "EOS = '<EOS>'\n",
    "UNK = '<UNK>'\n",
    "\n",
    "ixBOS = w_to_ix[BOS]\n",
    "ixEOS = w_to_ix[EOS]\n",
    "ixUNK = w_to_ix[UNK]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sents = [\n",
    "    'andrés manuel lópez obrador',\n",
    "    'lópez obrador',\n",
    "    'ricardo anaya',\n",
    "    'anaya',\n",
    "    'josé antonio meade',\n",
    "    'meade',\n",
    "    'el candidato',\n",
    "    '\"'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[7471, 64, 56, 29, 30],\n",
       " [7471, 29, 30],\n",
       " [7471, 49, 41],\n",
       " [7471, 41],\n",
       " [7471, 48, 50, 34],\n",
       " [7471, 34],\n",
       " [7471, 5, 26],\n",
       " [7471, 10]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sents_ix = [\n",
    "    [ixBOS]+[w_to_ix[w] for w in sent.split()] \n",
    "    for sent in test_sents\n",
    "]\n",
    "test_sents_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_pytorch_tensor(list_of_lists):\n",
    "    return [\n",
    "        torch.LongTensor(l).to(device)\n",
    "        for l in list_of_lists\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([7471,   64,   56,   29,   30], device='cuda:0'),\n",
       " tensor([7471,   29,   30], device='cuda:0'),\n",
       " tensor([7471,   49,   41], device='cuda:0'),\n",
       " tensor([7471,   41], device='cuda:0'),\n",
       " tensor([7471,   48,   50,   34], device='cuda:0'),\n",
       " tensor([7471,   34], device='cuda:0'),\n",
       " tensor([7471,    5,   26], device='cuda:0'),\n",
       " tensor([7471,   10], device='cuda:0')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = to_pytorch_tensor(test_sents_ix)\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_ix_sentence(sentence, end='\\n'):\n",
    "    print(' '.join(ix_to_w[ix] for ix in sentence.data), end=end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_sent(sentence, limit = 47, choose_max=True):\n",
    "    if len(sentence) == 0:\n",
    "        sentence = torch.LongTensor([ixBOS]).to(device)\n",
    "    torchEOS = torch.LongTensor([ixEOS]).to(device)\n",
    "    i = 0\n",
    "    prediccion = sentence[-1:]\n",
    "    while not torch.eq(prediccion, torchEOS):\n",
    "        out = model(sentence)\n",
    "        if choose_max:\n",
    "            prediccion = torch.argmax(out[-1:], dim=1)\n",
    "        else:\n",
    "            prediccion = torch.multinomial(torch.exp(out[-1]), 1)\n",
    "#         print_ix_sentence(prediccion, end=' ') # Imprimo el siguiente caracter\n",
    "        sentence = torch.cat((sentence, prediccion), dim=0) # Genero siguiente cadena\n",
    "        i+=1\n",
    "        if i>limit:\n",
    "            break\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================\n",
      "- Probando enunciado:\n",
      "<BOS> andrés manuel lópez obrador\n",
      "\n",
      "- Enunciado generado:\n",
      "<BOS> andrés manuel lópez obrador anunció que se <UNK> la elección del país , porque no se <UNK> el dinero de la gente , porque no se <UNK> el dinero de la gente , porque no se <UNK> el dinero de la gente , porque no se <UNK> el dinero de la gente\n",
      "\n",
      "\n",
      "===================================================\n",
      "- Probando enunciado:\n",
      "<BOS> lópez obrador\n",
      "\n",
      "- Enunciado generado:\n",
      "<BOS> lópez obrador expresó que se <UNK> la corrupción , porque no se <UNK> el dinero de la gente , porque no se <UNK> el dinero de la gente , porque no se <UNK> el dinero de la gente , porque no se <UNK> el dinero de la gente , porque\n",
      "\n",
      "\n",
      "===================================================\n",
      "- Probando enunciado:\n",
      "<BOS> ricardo anaya\n",
      "\n",
      "- Enunciado generado:\n",
      "<BOS> ricardo anaya cortés , el candidato de la coalición todos por méxico al frente a la presidencia de la república , josé antonio meade , dijo que se <UNK> la corrupción , porque no se <UNK> el dinero de la gente , porque no se <UNK> el dinero de la\n",
      "\n",
      "\n",
      "===================================================\n",
      "- Probando enunciado:\n",
      "<BOS> anaya\n",
      "\n",
      "- Enunciado generado:\n",
      "<BOS> anaya es que se <UNK> la corrupción , porque no se <UNK> el dinero de la gente , porque no se <UNK> el dinero de la gente , porque no se <UNK> el dinero de la gente , porque no se <UNK> el dinero de la gente , porque\n",
      "\n",
      "\n",
      "===================================================\n",
      "- Probando enunciado:\n",
      "<BOS> josé antonio meade\n",
      "\n",
      "- Enunciado generado:\n",
      "<BOS> josé antonio meade dijo que se <UNK> la elección del país , porque no se <UNK> el dinero de la gente , porque no se <UNK> el dinero de la gente , porque no se <UNK> el dinero de la gente , porque no se <UNK> el dinero de la gente\n",
      "\n",
      "\n",
      "===================================================\n",
      "- Probando enunciado:\n",
      "<BOS> meade\n",
      "\n",
      "- Enunciado generado:\n",
      "<BOS> meade afirmó que se <UNK> la elección del país , porque no se <UNK> el dinero de la gente , porque no se <UNK> el dinero de la gente , porque no se <UNK> el dinero de la gente , porque no se <UNK> el dinero de la gente\n",
      "\n",
      "\n",
      "===================================================\n",
      "- Probando enunciado:\n",
      "<BOS> el candidato\n",
      "\n",
      "- Enunciado generado:\n",
      "<BOS> el candidato de la coalición todos por méxico al frente a la presidencia de la república , josé antonio meade , dijo que se <UNK> la corrupción , porque no se <UNK> el dinero de la gente , porque no se <UNK> el dinero de la gente , porque no\n",
      "\n",
      "\n",
      "===================================================\n",
      "- Probando enunciado:\n",
      "<BOS> \"\n",
      "\n",
      "- Enunciado generado:\n",
      "<BOS> \" no se <UNK> la corrupción , porque no se <UNK> el dinero de la gente , porque no se <UNK> el dinero de la gente , porque no se <UNK> el dinero de la gente , porque no se <UNK> el dinero de la gente , porque no\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for sentence in X_test[:10]:\n",
    "        print('===================================================')\n",
    "        print('- Probando enunciado:')\n",
    "        print_ix_sentence(sentence)\n",
    "        print()\n",
    "        \n",
    "        print('- Enunciado generado:')\n",
    "        gen_sentence = generar_sent(sentence, limit=47, choose_max=True)\n",
    "        print_ix_sentence(gen_sentence)\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================\n",
      "- Probando enunciado:\n",
      "<BOS> andrés manuel lópez obrador\n",
      "\n",
      "- Enunciado generado:\n",
      "<BOS> andrés manuel lópez obrador anunció que abarrotaron de acceso gutiérrez , no quiere ver las pensiones , debido caminando fraude electoral está por el primer oposición de escasos recursos sus puntos de ricardo monreal \" , afirmó meade , con este encuentro en tamaulipas . <EOS>\n",
      "\n",
      "\n",
      "===================================================\n",
      "- Probando enunciado:\n",
      "<BOS> lópez obrador\n",
      "\n",
      "- Enunciado generado:\n",
      "<BOS> lópez obrador destacó que en su derecho a nivel nacional y muchas veces garantizado a quienes actos . <EOS>\n",
      "\n",
      "\n",
      "===================================================\n",
      "- Probando enunciado:\n",
      "<BOS> ricardo anaya\n",
      "\n",
      "- Enunciado generado:\n",
      "<BOS> ricardo anaya destacó que se <UNK> para hacer un programa de calidad para las elecciones . <EOS>\n",
      "\n",
      "\n",
      "===================================================\n",
      "- Probando enunciado:\n",
      "<BOS> anaya\n",
      "\n",
      "- Enunciado generado:\n",
      "<BOS> anaya está abierto muy bravo . <EOS>\n",
      "\n",
      "\n",
      "===================================================\n",
      "- Probando enunciado:\n",
      "<BOS> josé antonio meade\n",
      "\n",
      "- Enunciado generado:\n",
      "<BOS> josé antonio meade puntualizó que se generarán jóvenes en la materia y para convertir es que se utilizan el primero del tribunal electoral de un mes sin una encuesta , las que no han cargos solidario para enfrentar el tráfico a empresarios \" . <EOS>\n",
      "\n",
      "\n",
      "===================================================\n",
      "- Probando enunciado:\n",
      "<BOS> meade\n",
      "\n",
      "- Enunciado generado:\n",
      "<BOS> meade asumió la próxima educativa , el aspirante presidencial afirmó que antes el presupuesto de calidad y el compromiso de trabajo y apoyos . <EOS>\n",
      "\n",
      "\n",
      "===================================================\n",
      "- Probando enunciado:\n",
      "<BOS> el candidato\n",
      "\n",
      "- Enunciado generado:\n",
      "<BOS> el candidato de la coalición todos por méxico dijo que no los siguientes ministros del pueblo y apoyar al público que la necesita de lo que viene de todo , el secretario de los programas de ejidatarios al medio ambiente tercio de tiempo completo con su valor , ahora resulta\n",
      "\n",
      "\n",
      "===================================================\n",
      "- Probando enunciado:\n",
      "<BOS> \"\n",
      "\n",
      "- Enunciado generado:\n",
      "<BOS> \" no importa al productor , sino hasta una intención del cambio \" , indicó . <EOS>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for sentence in X_test[:10]:\n",
    "        print('===================================================')\n",
    "        print('- Probando enunciado:')\n",
    "        print_ix_sentence(sentence)\n",
    "        print()\n",
    "        \n",
    "        print('- Enunciado generado:')\n",
    "        gen_sentence = generar_sent(sentence, limit=47, choose_max=False)\n",
    "        print_ix_sentence(gen_sentence)\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardo los embeddings en un archivo csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.0578452 ,  0.3161479 , -0.5756407 , ...,  1.5266542 ,\n",
       "         1.2428318 ,  0.19398741],\n",
       "       [-2.112738  ,  0.60530764, -1.0337901 , ...,  1.2791759 ,\n",
       "         1.0066867 , -0.4245396 ],\n",
       "       [ 0.07019642, -0.3957354 ,  1.0859576 , ...,  0.77084315,\n",
       "        -0.9025867 , -0.08432264],\n",
       "       ...,\n",
       "       [ 1.088267  , -0.25628954,  0.8836263 , ..., -0.14433354,\n",
       "        -0.63452756, -0.6571088 ],\n",
       "       [-1.1417605 ,  1.2172513 ,  0.1527519 , ..., -0.81698406,\n",
       "        -1.0173255 ,  0.7318495 ],\n",
       "       [ 0.5671076 , -0.93567675, -0.49270746, ..., -0.51932335,\n",
       "        -0.04244279, -0.68007064]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = model.embedding.weight.detach().to('cpu').numpy()\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(embeddings).to_csv(\n",
    "    'embeddings.csv', \n",
    "    sep='\\t', \n",
    "    header=False,\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('aypit-2020-2': conda)",
   "language": "python",
   "name": "python37764bitaypit20202conda2eb9642ad5e346f38de29b14c14576bc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
