{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings con LSTM\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import Counter\n",
    "import random\n",
    "from itertools import chain\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "c:\\Users\\Usuario.000\\Documents\\Facultad\\Git\\2020-2\\APIT-2020-2\\ProyectoFinal\n"
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.boletines import get_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "c:\\Users\\Usuario.000\\Documents\\Facultad\\Git\\2020-2\\APIT-2020-2\\ProyectoFinal\\Embeddings\n"
    }
   ],
   "source": [
    "cd Embeddings/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición de constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Random Seed:  42069\n"
    }
   ],
   "source": [
    "# Número de GPUs disponibles. Usar 0 para modo CPU.\n",
    "ngpu = 1\n",
    "\n",
    "# Semilla a usar en los generadores de números aleatorios\n",
    "SEED = 42069\n",
    "# SEED = random.randint(1, 10000) # En caso de requerir más resultados\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "print(\"Random Seed: \", SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecciono el tipo de dispositivo a utilizar (gpu o cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "device(type='cuda', index=0)"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "# Decide si queremos correr en gpu o cpu\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtener corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "uri = open('./../mongo_uri.txt', 'r', encoding='utf-8').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "RAC\n1. De_la_mano_de jóvenes innovadores , comienza Ricardo_Anaya el camino hacia la Presidencia_de_la_República .\n377. Finalmente , sostuvo que López_Obrador es “ el espanta inversiones ” , porque su actitud provoca que estas se vayan , lo cual es muy delicado , porque cuando no hay inversión no crece la economía y no se generan empleos .\n753. “ Sí , necesitamos cambiar ciertas piezas de el modelo económico , estamos en un momento crítico , donde tenemos que elegir entre volver a el pasado a_través_de un cómic o construir un futuro más incluyente , igualitario , a_partir_de propuestas series , razonadas , en donde los cómo están claros y los para qué , son parte principal de lo que se pretende hacer ” , enfatizó a el contrastar con la propuesta que hace López_Obrador .\n1129. Yo quiero paz para Jalisco , yo quiero paz para México , y esa paz la voy a construir con las mujeres , esa paz la vamos a construir todos juntos ” , enfatizó .\n\nAMLO\n1. Asegura AMLO que en campaña todo será amor y paz , que los otros candidatos se ahorren sus provocaciones .\n377. “ Y tiene que ver con la alianza de el PRI y de el PAN , no hay que olvidar que los de el PRI , los de el PAN , los de el PRIAN se pusieron de acuerdo para nombrar a los consejeros de el INE , que tampoco son blancas palomas y también se pusieron de acuerdo para nombrar a los magistrados de el Tribunal_Electoral , yo lo denuncié en su momento ” , comentó .\n753. “ Antes en las elecciones pasadas no podíamos defender nos frente_a los ataques a la guerra sucia , a las calumnias , porque nos cercaban , nos daban espacios o actuaban de manera tendenciosa en los medios convencionales , pero en esta elección la diferencia la están marcando las redes sociales , ya no pueden ” , expresó .\n1129. En otro orden de ideas , informó que si hoy fuesen las elecciones ganaría con 25 puntos de ventaja , es bastante el número de mexicanos que lo apoyan , pero hay quienes crearán de manera artificial la polarizar el voto para las elecciones de el 1_de_julio , pero no es lo mismo el 2018 a el 2006 .\n1505. “ Cuánto fue el dinero de el presupuesto que se utilizó para imponer la mal llamada reforma educativa , es una investigación periodística de primer orden , salió hace dos_días en un periódico de que el secretario de Educación ( Aurelio_Nuño ) que se gastó 2_mil_millones_de_pesos , pero eso no es todo , se trata de un año ” , expuso .\n1881. En Autlán_de_Navarro , Jalisco , indicó que se tiene que respetar a los ganadores de la contienda electoral en relación a la Presidencia_de_la_República , pero también a quienes vayan como diputados locales y federales , así_como senadores , presidentes municipales .\n2257. Expresó que se mejorarán los servicios de salud como el ISSSTE y el Seguro , se aumentará a el doble la pensión para adultos mayores será de mil_500_pesos mensuales y para todos , y habrá apoyo para los discapacitados pobres de el país .\n2633. Adelanta AMLO que planteará a Trump un acuerdo de cooperación entre EU , Canadá , México y Centroamérica .\n3009. Indicó que ahora se compra la mitad de la luz que se consume en México a empresas extrajeras que tienen subsidio de 50_mil_millones_de_pesos a el año .\n\nJAMK\n1. José_Antonio_Meade fue recibido en Culiacán por cientos de simpatizantes sinaloenses .\n377. El aspirante presidencial se comprometió a mejorar la infraestructura carretera e hidráulica en la entidad .\n753. “ Si tuviera que resumir en una idea el país por el que apuesto para los próximos seis años , lo resumiría en tres palabras : un México líder ” , indicó .\n1129. De_acuerdo_con diversas estimaciones , las armas en poder de la delincuencia organizada en México podrían alcanzar el número de 1.5 millones de piezas , esto es 3 veces el arsenal con el que cuenta , por_ejemplo , el ejército de Guatemala .\n1505. Expresó que , sin_ninguna_duda y con toda certeza , logrará la victoria el 1_de_julio .\n1881. Dijo que con Mikel_Arriola va por una Ciudad_de_México donde abramos la llave y salga agua , que no sea extorsionada por delegados que condicionan el agua a la militancia política , una ciudad que no tandee el agua , donde el derecho a el agua sea vigente , se eviten las fugas y se evite la corrupción .\n\n"
    }
   ],
   "source": [
    "sentences = get_sentences(uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desenvuelvo los grupos de enunciados en un único arreglo con todos los enunciados de todos los candidatos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = list(chain(*sentences.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paso los tokens a minúsculas para reducir el tamaño del vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "El corpus consta de 6889 enunciados.\n"
    }
   ],
   "source": [
    "corpus = [[w.lower() for w in sent] for sent in corpus]\n",
    "print(f'El corpus consta de {len(corpus)} enunciados.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecciono k oraciones de prueba para el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "promete amlo que no cancelará , ni modificará las concesiones en radio y televisión .\nademás , las estancias que atiendan a ciertos sectores económicos como las maquiladoras , tendrán horario nocturno .\nqueremos externarle a andrés_manuel que no pierda su buen sentido de el humor , porque , andrés_manuel , ¡ vas a volver a perder ! ” , subrayó meade ante habitantes de la región de los_altos_de_jalisco , quienes se unieron en una sola voz para apoyar a su candidato con la porra ¡ pepe presidente !\nmeade congrega a más de 60_mil simpatizantes durante los eventos de el fin de semana .\ny creo que la conclusión es que no ha fallado la gente , han fallado los gobiernos , porque hemos tenido gobiernos profundamente corruptos y gobiernos profundamente ineficaces y eso es lo que yo he querido cambiar ” .\njosé_antonio_meade impulsará el desarrollo integral de niñas , niños y adolescentes , dijo .\neste programa , detalló , consiste en acciones concretas para la primera infancia , desde la gestación y hasta los primeros dos años de vida , con acciones de estimulación temprana , principalmente a_través_de el cariño y cuidado de los padres , una adecuada alimentación y un sistema de salud para el correcto crecimiento de las niñas y de los niños .\nsostuvo que hará su trabajo para que haya justicia en esta región de chiapas y en todo el país .\njosé_antonio_meade arrancará su campaña presidencial en mérida , desde donde comenzará un recorrido por todo el país para avanzar , junto_con la ciudadanía , en la construcción de un méxico más justo y equitativo .\nsobre si cuidará su cartera con la reunión de los hombres de negocios , lópez_obrador dijo que la cuidó en el debate , porque tiene que cuidar la cartera , “ aquí la traigo ” .\n"
    }
   ],
   "source": [
    "k = 1000\n",
    "sentences = random.sample(corpus, k=k)\n",
    "print('\\n'.join([' '.join(sent) for sent in sentences[::k//10]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexar Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuento los tokens en todas las oraciones e imprimo los 40 más frecuentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[('toma', 1), ('votarán', 1), ('escribió', 1), ('conciencia', 1), ('plurinominal', 1), ('canadiense', 1), ('adquirido', 1), ('55_millones_de_dólares', 1), ('señalado', 1), ('napoleón_gomez_urrutia', 1), ('contento', 1), ('personalizada', 1), ('formalización', 1), ('misma', 1), ('mueve', 1), ('siglos', 1), ('pasan', 1), ('repite', 1), ('usual', 1), ('interesantes', 1), ('productos', 1), ('comuneros', 1), ('bloqueó', 1), ('vano', 1), ('papantla', 1), ('amecameca', 1), ('slim', 1), ('saliera', 1), ('carlos_slim', 1), ('carlos_salinas', 1), ('posiblemente', 1), ('geografía', 1), ('mapa', 1), ('visita', 1), ('diputado', 1), ('comité_directivo_estatal_de_el_pri', 1), ('presiente', 1), ('mexe', 1), ('reabrir', 1)]\n"
    }
   ],
   "source": [
    "freq = sum([Counter(sent) for sent in sentences], Counter())\n",
    "print(freq.most_common()[:-40:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "5269"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "num_tokens = len(freq.keys())\n",
    "num_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agrego un diccionario para pasar de palabra a índice numérico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_to_index = {\n",
    "    w : ix\n",
    "    for ix, (w, freq) in enumerate(freq.most_common())\n",
    "    if freq > 1 # No toma en cuenta los hapax\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS = '<BOS>'\n",
    "EOS = '<EOS>'\n",
    "UNK = '<UNK>'\n",
    "\n",
    "ixBOS = num_tokens + 1\n",
    "ixEOS = num_tokens + 2\n",
    "ixUNK = num_tokens + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_to_index[BOS] = ixBOS\n",
    "w_to_index[EOS] = ixEOS\n",
    "w_to_index[UNK] = ixUNK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creo el diccionario inverso, para convertir de índices a palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_w = [ w for w, ix in w_to_index.items() ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tests = random.sample(w_to_index.keys(), k=10)\n",
    "# for w in tests:\n",
    "#     ix = w_to_index[w]\n",
    "#     assert index_to_w[ix] == w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexo todo el corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w_to_index_unk(w):\n",
    "    \"\"\"\n",
    "    Le asigna el token UNK a palabras que no aparezcan en el corpus\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return w_to_index[w] \n",
    "    except KeyError:\n",
    "        return ixUNK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Ejemplos X\n[array([5272,  239,    2,   11,  731,    0,   53, 1472,   19, 5272,    8,\n       1473,    7,  875]), array([   8,  208,    8, 5272,    0,  330,    2,    3,   26,    1,    3,\n        110,    0,   46,    7, 1474,    9, 1475,   48,  218,    7,  170,\n          2,   54,   20,  160,    0,   54,  219,    1,    4,  161,    1,\n          3,  111]), array([ 124,   16,    4, 1476,    2,   31,    0,    2, 5272,   34,  331,\n          2,   44,    5,  732,    5,  473,   15,    0,   35, 1477])]\nEjemplos Y\n[array([ 239,    2,   11,  731,    0,   53, 1472,   19, 5272,    8, 1473,\n          7,  875,    6]), array([ 208,    8, 5272,    0,  330,    2,    3,   26,    1,    3,  110,\n          0,   46,    7, 1474,    9, 1475,   48,  218,    7,  170,    2,\n         54,   20,  160,    0,   54,  219,    1,    4,  161,    1,    3,\n        111,    6]), array([  16,    4, 1476,    2,   31,    0,    2, 5272,   34,  331,    2,\n         44,    5,  732,    5,  473,   15,    0,   35, 1477,    6])]\n"
    }
   ],
   "source": [
    "sentences_ix = [\n",
    "    [ w_to_index_unk(w) for w in sent ] \n",
    "    for sent in sentences\n",
    "]\n",
    "\n",
    "X = [ np.asarray(sent[:-1]) for sent in sentences_ix ]\n",
    "Y = [ np.asarray(sent[1:]) for sent in sentences_ix ] \n",
    "\n",
    "print('Ejemplos X')\n",
    "print(X[:3])\n",
    "print('Ejemplos Y')\n",
    "print(Y[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mando los vectores de entrada y salida a tensores en gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_pytorch_tensor(list_of_lists):\n",
    "    return [\n",
    "        torch.from_numpy(l).long().to(device)\n",
    "        for l in list_of_lists\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = to_pytorch_tensor(X_train)\n",
    "X_test = to_pytorch_tensor(X_test)\n",
    "Y_train = to_pytorch_tensor(Y_train)\n",
    "Y_test = to_pytorch_tensor(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo\n",
    "\n",
    "### 1. Capa de embedding\n",
    "\n",
    "### 2. Capa oculta\n",
    "\n",
    "### 3. Capa de salida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defino las variables para la red neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensión de entrada (one-hot), tamaño del vocabulario\n",
    "D_in = len(index_to_w)\n",
    "\n",
    "# Dimensión de la capa de embedding\n",
    "D_emb = 16 # 32\n",
    "\n",
    "# Dimensión de la capa lstm\n",
    "D_lstm = 8 # 16\n",
    "\n",
    "# Dimensión de la capa de salida\n",
    "D_out = D_in\n",
    "\n",
    "# Épocas de entrenamiento\n",
    "num_epochs = 10\n",
    "\n",
    "# Learning rate\n",
    "lr = 0.001\n",
    "\n",
    "# Betas para Adam\n",
    "beta1 = 0.0002\n",
    "beta2 = 0.999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, ngpu, D_in, D_emb, D_lstm, D_out):\n",
    "        super(Model, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.embedding = nn.Embedding(num_embeddings=D_in, embedding_dim=D_emb)#, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(input_size=D_emb, hidden_size=D_lstm, bias=True)#, batch_first=True)\n",
    "        # self.out_layer = nn.Sequential(\n",
    "        #     nn.Embedding(num_embeddings=D_in, embedding_dim=D_emb, padding_idx=0),\n",
    "        #     nn.LSTM(input_size=D_emb, hidden_size=D_lstm, bias=True, batch_first=True),\n",
    "        #     nn.Linear(in_features=D_lstm, out_features=D_out, bias=True),\n",
    "        #     nn.Softmax(dim=D_out)\n",
    "        # )\n",
    "        self.linear = nn.Linear(in_features=D_lstm, out_features=D_out, bias=True)\n",
    "        # self.out_layer = nn.Softmax(dim=D_out)\n",
    "        # self.out_layer = nn.Sequential(\n",
    "        #     nn.Linear(in_features=D_lstm, out_features=D_out, bias=True),\n",
    "        #     nn.Softmax(dim=D_out)\n",
    "        # )\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        T = len(sentence)\n",
    "        # print(T)\n",
    "        embeddings = self.embedding(sentence).view(T, 1, -1)\n",
    "        # print('Embeddings')\n",
    "        # print(embeddings)\n",
    "        lstm_out, (ht, ct) = self.lstm(embeddings)\n",
    "        lstm_out = lstm_out.view(T, -1)\n",
    "        # print('LSTM_out')\n",
    "        # print(lstm_out)\n",
    "        preact_out = self.linear(lstm_out).view(T, -1)\n",
    "        # print('Preact out')\n",
    "        # print(preact_out)\n",
    "        return F.log_softmax(preact_out, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Model(\n  (embedding): Embedding(2292, 16)\n  (lstm): LSTM(16, 8)\n  (linear): Linear(in_features=8, out_features=2292, bias=True)\n)"
     },
     "metadata": {},
     "execution_count": 107
    }
   ],
   "source": [
    "model = Model(ngpu, D_in, D_emb, D_lstm, D_out).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Inicialización de pesos\n",
    "# def weights_init(m):\n",
    "#     classname = m.__class__.__name__\n",
    "#     if classname.find('Embedding') != -1:\n",
    "#         # Regularizo los pesos\n",
    "#         n = m.num_embeddings\n",
    "#         y = 1.0/np.sqrt(n)\n",
    "#         m.weight.data.uniform_(-y, y)\n",
    "#     elif classname.find('Linear') != -1:\n",
    "#         n = m.in_features\n",
    "#         y = 1.0/np.sqrt(n)\n",
    "#         m.weight.data.uniform_(-y, y)\n",
    "#         m.bias.data.fill_(0)\n",
    "#     elif classname.find('LSTM') != -1:\n",
    "#         n = m.input_size\n",
    "#         y = 1.0/np.sqrt(n)\n",
    "#         m.weight_ih_l.data.fill_(0)\n",
    "#         m.weight_hh_l.data.uniform_(-y, y)\n",
    "#         m.bias_ih_l.data.fill_(0)\n",
    "        # m.bias_hh_l.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.apply(weights_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entropía cruzada como optmizador y SGD como optimizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Embeddings\ntensor([[[-1.7881e-01, -4.5229e-01, -9.4707e-01, -3.7844e-01, -3.0049e-01,\n           1.7676e+00,  1.8660e-01,  8.9402e-01, -2.9348e-01, -3.2718e-01,\n           5.9819e-01, -1.3414e-01,  7.3961e-01, -8.5819e-01, -1.0146e-02,\n           2.0443e-01]],\n\n        [[-6.7835e-01, -9.1612e-01,  8.7945e-01,  3.9427e-02, -9.7080e-02,\n           8.5375e-01,  1.0209e+00, -2.0906e+00, -1.1392e+00, -3.3436e-02,\n          -9.5894e-01,  9.1496e-01,  3.0243e-01,  3.8461e-01, -2.3871e-01,\n           8.2738e-02]],\n\n        [[-1.6273e+00,  1.4199e+00,  3.5919e-01, -4.7290e-01, -5.1169e-01,\n          -7.7602e-01,  2.2123e+00,  7.3892e-01,  7.6709e-01, -2.4265e-01,\n           9.2501e-02,  6.1254e-02, -6.7082e-02, -6.4032e-01, -3.4414e-01,\n          -6.2533e-01]],\n\n        [[ 1.1072e+00, -6.2959e-01,  1.5976e+00, -1.0305e+00, -2.0247e+00,\n           1.4729e+00,  6.8229e-01,  9.6242e-01,  1.3576e+00, -1.5342e+00,\n           2.5808e-01,  6.1571e-01,  6.6180e-01, -1.2623e+00, -1.2223e+00,\n          -1.0286e+00]],\n\n        [[ 8.3000e-01,  2.0730e-01, -6.9239e-01,  1.7111e-01, -8.1992e-01,\n           1.2830e+00,  1.6806e+00,  1.1355e+00, -1.0439e+00, -1.6043e+00,\n          -9.7539e-01,  3.9566e-01,  5.1412e-01, -5.6854e-01, -2.1379e+00,\n           6.8877e-01]],\n\n        [[ 1.5073e-01,  1.0457e+00,  9.0697e-01,  3.0394e+00, -2.1205e+00,\n          -5.7290e-01, -6.8521e-01,  4.7044e-01, -5.1097e-01, -7.1725e-01,\n          -1.1246e-01, -3.2821e-01,  1.8596e+00, -8.4703e-02, -4.5925e-01,\n           1.3872e+00]],\n\n        [[ 9.3675e-01, -7.2473e-01, -5.9262e-02, -1.5379e+00, -1.1964e+00,\n          -6.4501e-01,  1.5759e-01, -2.1482e-02,  1.0373e+00,  1.0561e+00,\n           6.1421e-02,  5.0943e-01, -3.8396e-02, -2.0337e-01,  2.6209e-01,\n          -1.1906e+00]],\n\n        [[ 1.1721e+00, -1.1797e+00, -2.2193e+00,  1.9147e+00,  7.1799e-01,\n           1.9187e+00,  9.8880e-01,  1.7853e-01, -1.2285e+00, -8.0009e-01,\n           2.1283e+00, -5.2804e-01, -9.2800e-01, -7.0254e-01, -1.3266e+00,\n          -7.2877e-01]],\n\n        [[ 1.3167e+00, -1.3618e-01,  6.4574e-01, -7.6891e-01, -5.4477e-01,\n          -6.4321e-01,  1.5606e+00, -1.0717e+00, -1.8973e+00,  2.8927e-01,\n           1.0275e+00, -4.2114e-01, -1.5300e+00,  1.0483e-01, -1.2322e+00,\n           1.0944e+00]],\n\n        [[ 5.2197e-01,  5.4619e-01,  1.0814e+00, -1.0035e+00,  6.8451e-01,\n           5.5341e-01, -3.2371e-01, -1.5162e+00, -5.3619e-01, -5.6006e-01,\n          -1.5100e+00, -5.6108e-01, -3.2972e-01, -1.5038e-01,  2.0761e+00,\n          -4.6470e-01]],\n\n        [[ 6.4023e-01, -7.5434e-01, -1.5827e-01,  5.4063e-01,  2.3325e-02,\n           6.3961e-01, -9.4473e-02, -1.0192e+00,  1.7405e+00,  1.1686e+00,\n          -9.9353e-01, -2.3745e-02, -7.3197e-01,  1.3369e+00,  1.1372e-02,\n          -2.0202e+00]],\n\n        [[-1.1589e-01, -1.2488e+00,  3.0896e-01,  2.2074e-03, -1.0996e+00,\n           1.5187e+00, -6.8886e-01,  1.1079e+00,  3.3759e-01,  1.5094e+00,\n           1.0317e+00, -3.1509e-01, -7.4042e-01,  1.8034e+00, -7.4690e-01,\n          -7.3808e-01]],\n\n        [[-1.8484e-01,  1.0193e+00,  1.5686e+00, -1.2210e+00,  3.6280e-01,\n           3.7247e-01,  1.8457e-02, -2.0998e-01, -1.7194e+00, -3.4713e-01,\n           1.4854e+00,  3.6902e-01,  5.8960e-02,  4.4983e-01, -7.0851e-01,\n           6.5179e-01]],\n\n        [[-2.2625e-01, -1.0990e+00,  3.6151e-01,  4.6127e-01,  7.8901e-01,\n           6.0624e-01, -2.0503e-01,  1.6875e+00,  1.1758e+00, -6.6687e-01,\n          -4.7547e-01,  1.5936e-01, -1.5961e+00,  8.7253e-01,  8.2861e-02,\n          -2.2026e-01]],\n\n        [[ 9.3675e-01, -7.2473e-01, -5.9262e-02, -1.5379e+00, -1.1964e+00,\n          -6.4501e-01,  1.5759e-01, -2.1482e-02,  1.0373e+00,  1.0561e+00,\n           6.1421e-02,  5.0943e-01, -3.8396e-02, -2.0337e-01,  2.6209e-01,\n          -1.1906e+00]],\n\n        [[-9.9491e-03, -3.9601e-02, -5.0087e-01, -1.9991e-01, -6.6491e-01,\n           8.6389e-01,  1.2649e+00, -2.6138e+00, -4.2394e-01,  1.0105e+00,\n           1.8058e+00, -7.7632e-01, -3.4985e-03,  4.7598e-01,  2.2400e-01,\n          -1.0525e-01]],\n\n        [[ 4.9940e-01, -4.5095e-01,  8.2564e-01, -1.0731e+00,  1.1141e+00,\n          -5.7389e-01,  9.4401e-01,  1.5995e+00,  4.8151e-02,  2.1716e+00,\n           1.8082e-01,  5.3290e-01, -2.8663e-01, -5.3155e-01, -1.5796e+00,\n          -7.3330e-01]],\n\n        [[ 5.2069e-01, -4.3581e-01, -8.0779e-01, -8.3689e-01, -1.1242e+00,\n           1.6822e-01,  1.7303e-01, -2.1992e-01, -9.8062e-01,  1.3884e+00,\n           2.2815e-01,  1.2478e+00, -1.1400e+00,  6.5643e-01, -1.2428e+00,\n          -1.4781e+00]],\n\n        [[ 1.7931e+00,  9.3211e-01, -1.1234e+00,  2.2066e+00, -2.4651e-01,\n          -7.4385e-01, -1.1085e-01,  1.2452e+00,  1.3543e+00,  5.1995e-01,\n          -1.0817e+00,  8.4109e-01, -8.8759e-01, -1.3958e+00, -5.5090e-02,\n          -1.4835e+00]],\n\n        [[ 3.1380e-01,  6.8985e-01,  8.9933e-01,  8.4976e-01, -5.1373e-01,\n           1.4003e-01,  7.3128e-01, -6.2799e-01, -6.6955e-04,  1.3419e+00,\n           1.3562e+00, -1.1115e+00,  3.9431e-01,  3.8353e+00, -1.1305e-02,\n           1.6223e-01]],\n\n        [[ 7.8867e-01, -6.2411e-01, -3.1616e-01, -1.7325e+00, -1.7334e-01,\n           1.3439e+00, -8.1127e-01,  1.6283e-01, -2.9669e+00, -1.9558e+00,\n           1.1224e+00, -2.8185e-01,  1.3830e+00,  1.6482e+00,  4.2942e-01,\n          -8.7708e-01]],\n\n        [[ 1.1072e+00, -6.2959e-01,  1.5976e+00, -1.0305e+00, -2.0247e+00,\n           1.4729e+00,  6.8229e-01,  9.6242e-01,  1.3576e+00, -1.5342e+00,\n           2.5808e-01,  6.1571e-01,  6.6180e-01, -1.2623e+00, -1.2223e+00,\n          -1.0286e+00]],\n\n        [[-8.0482e-01, -1.4422e+00, -6.2725e-01, -6.6132e-01, -3.6003e-01,\n          -5.0650e-01, -1.0373e+00, -5.7495e-02,  2.4416e+00, -7.4070e-01,\n          -8.2429e-01, -5.5365e-01, -1.1267e-01, -4.6130e-01, -9.1060e-02,\n           7.6374e-01]],\n\n        [[ 1.5073e-01,  1.0457e+00,  9.0697e-01,  3.0394e+00, -2.1205e+00,\n          -5.7290e-01, -6.8521e-01,  4.7044e-01, -5.1097e-01, -7.1725e-01,\n          -1.1246e-01, -3.2821e-01,  1.8596e+00, -8.4703e-02, -4.5925e-01,\n           1.3872e+00]],\n\n        [[ 1.1072e+00, -6.2959e-01,  1.5976e+00, -1.0305e+00, -2.0247e+00,\n           1.4729e+00,  6.8229e-01,  9.6242e-01,  1.3576e+00, -1.5342e+00,\n           2.5808e-01,  6.1571e-01,  6.6180e-01, -1.2623e+00, -1.2223e+00,\n          -1.0286e+00]],\n\n        [[-5.3773e-01, -2.7404e-02,  8.9132e-02, -1.5422e+00, -7.2210e-01,\n           4.1922e-01,  6.6971e-01,  9.8421e-01,  1.2725e+00,  1.5195e-01,\n          -1.3602e+00,  7.9729e-01,  2.0476e-03,  4.3838e-01,  6.8945e-01,\n          -3.8806e-01]],\n\n        [[ 5.4462e-01,  5.3161e-02,  9.0911e-01,  1.2033e+00, -1.7989e-01,\n           8.2793e-01, -6.5763e-01,  1.3203e-01, -8.3749e-02,  7.2575e-01,\n          -8.2937e-01, -3.2734e-01,  5.0739e-01, -2.5245e-01,  2.1792e+00,\n          -3.4730e-01]],\n\n        [[-6.9196e-01,  6.4993e-01, -8.4569e-01, -2.5376e-01, -8.9631e-01,\n          -1.0915e+00,  3.2807e-01, -1.4392e-01, -8.5853e-01,  1.0971e+00,\n          -1.4342e+00,  1.0775e+00, -1.3375e-01,  3.6291e-01,  2.4595e-01,\n          -1.7472e-01]],\n\n        [[-7.4452e-01,  9.9979e-01,  5.0418e-01,  1.0659e-01, -1.0107e+00,\n           8.7213e-01,  1.0817e+00, -2.5872e+00,  8.5277e-01,  1.2178e+00,\n           1.2322e+00, -6.6501e-01,  1.7070e+00, -8.9731e-01,  1.2215e-01,\n          -1.0770e+00]],\n\n        [[ 9.3675e-01, -7.2473e-01, -5.9262e-02, -1.5379e+00, -1.1964e+00,\n          -6.4501e-01,  1.5759e-01, -2.1482e-02,  1.0373e+00,  1.0561e+00,\n           6.1421e-02,  5.0943e-01, -3.8396e-02, -2.0337e-01,  2.6209e-01,\n          -1.1906e+00]],\n\n        [[ 1.8676e+00, -2.8436e-01, -5.4931e-01,  1.1333e+00,  1.2195e+00,\n           9.5027e-01,  1.4979e+00,  1.2265e+00, -8.1928e-03, -2.5199e+00,\n           2.1707e+00, -5.6266e-01, -2.2331e-01,  2.7353e-01, -2.8557e-01,\n           7.4483e-02]],\n\n        [[ 9.3675e-01, -7.2473e-01, -5.9262e-02, -1.5379e+00, -1.1964e+00,\n          -6.4501e-01,  1.5759e-01, -2.1482e-02,  1.0373e+00,  1.0561e+00,\n           6.1421e-02,  5.0943e-01, -3.8396e-02, -2.0337e-01,  2.6209e-01,\n          -1.1906e+00]],\n\n        [[ 1.5073e-01,  1.0457e+00,  9.0697e-01,  3.0394e+00, -2.1205e+00,\n          -5.7290e-01, -6.8521e-01,  4.7044e-01, -5.1097e-01, -7.1725e-01,\n          -1.1246e-01, -3.2821e-01,  1.8596e+00, -8.4703e-02, -4.5925e-01,\n           1.3872e+00]],\n\n        [[-1.2325e+00,  7.5704e-01,  5.0793e-01,  5.9058e-01, -7.7397e-01,\n          -5.3689e-01,  7.0613e-01, -5.8928e-01, -9.2037e-01,  5.5872e-01,\n           1.9242e+00,  1.0196e-01,  1.4182e+00,  1.0028e+00,  7.0431e-01,\n           1.1035e+00]],\n\n        [[-2.6723e-01, -8.2829e-01,  6.5549e-01, -1.5673e-01, -2.0798e-01,\n          -1.8150e+00,  8.1715e-02, -8.7859e-01,  5.8057e-01,  7.6824e-01,\n           1.0031e+00,  3.1153e-01, -1.7579e+00, -9.8734e-01, -1.7367e-01,\n           5.6536e-01]],\n\n        [[-1.2325e+00,  7.5704e-01,  5.0793e-01,  5.9058e-01, -7.7397e-01,\n          -5.3689e-01,  7.0613e-01, -5.8928e-01, -9.2037e-01,  5.5872e-01,\n           1.9242e+00,  1.0196e-01,  1.4182e+00,  1.0028e+00,  7.0431e-01,\n           1.1035e+00]],\n\n        [[ 2.2710e+00, -5.0476e-01,  5.5839e-01,  1.7585e+00,  1.3412e+00,\n          -2.3578e-01, -1.3896e+00, -2.6334e-01, -3.2037e-01,  1.4948e+00,\n           1.6626e+00, -6.7819e-02, -3.7955e-01,  6.5835e-01, -1.0092e+00,\n          -3.4415e-01]],\n\n        [[-7.4240e-01,  1.2153e+00, -9.4245e-02,  7.0482e-01, -1.9226e+00,\n          -9.3610e-01,  1.0369e+00,  4.8163e-01,  4.4820e-01, -1.8863e-01,\n           2.7153e+00,  8.8409e-02, -1.8009e-01,  2.5054e-01,  1.9242e-01,\n          -1.1302e+00]],\n\n        [[-4.0008e-01,  2.1466e-01,  8.6400e-01,  1.0727e+00, -2.4309e+00,\n           7.1460e-01, -2.7353e-01,  9.1449e-01, -6.1333e-01,  2.7456e-01,\n          -1.7897e+00,  2.0556e+00, -5.3668e-01, -4.5803e-01,  1.5593e+00,\n          -6.0172e-01]],\n\n        [[-1.6273e+00,  1.4199e+00,  3.5919e-01, -4.7290e-01, -5.1169e-01,\n          -7.7602e-01,  2.2123e+00,  7.3892e-01,  7.6709e-01, -2.4265e-01,\n           9.2501e-02,  6.1254e-02, -6.7082e-02, -6.4032e-01, -3.4414e-01,\n          -6.2533e-01]],\n\n        [[ 5.2069e-01, -4.3581e-01, -8.0779e-01, -8.3689e-01, -1.1242e+00,\n           1.6822e-01,  1.7303e-01, -2.1992e-01, -9.8062e-01,  1.3884e+00,\n           2.2815e-01,  1.2478e+00, -1.1400e+00,  6.5643e-01, -1.2428e+00,\n          -1.4781e+00]],\n\n        [[-8.6065e-01, -9.4139e-01, -4.3191e-01, -1.6046e+00,  4.0510e-01,\n          -9.6969e-02,  1.7271e+00, -4.2138e-01, -1.2118e+00,  1.8144e-01,\n          -8.9429e-01, -1.1420e+00,  3.1742e-01, -3.8695e-01, -2.1381e+00,\n          -3.6598e-01]],\n\n        [[-3.6590e-02, -8.8239e-02, -2.6495e-01,  3.1795e-01,  8.6834e-02,\n          -1.6365e-02,  2.9911e-01,  1.7107e-01, -1.7017e-01,  9.0746e-02,\n           6.7254e-02,  1.1593e-01, -1.5847e-01, -3.3404e-02, -1.5866e-01,\n           1.7906e-01]],\n\n        [[ 1.1072e+00, -6.2959e-01,  1.5976e+00, -1.0305e+00, -2.0247e+00,\n           1.4729e+00,  6.8229e-01,  9.6242e-01,  1.3576e+00, -1.5342e+00,\n           2.5808e-01,  6.1571e-01,  6.6180e-01, -1.2623e+00, -1.2223e+00,\n          -1.0286e+00]],\n\n        [[-3.6645e-01, -1.3767e-02,  8.2987e-02, -8.5814e-03, -1.4223e+00,\n           2.1793e-01, -4.9926e-01,  1.4727e+00, -3.6399e-01, -1.8636e+00,\n          -1.0047e+00,  1.4876e-02,  9.4714e-02,  1.0178e+00,  4.6986e-01,\n          -2.4920e-01]],\n\n        [[ 1.5073e-01,  1.0457e+00,  9.0697e-01,  3.0394e+00, -2.1205e+00,\n          -5.7290e-01, -6.8521e-01,  4.7044e-01, -5.1097e-01, -7.1725e-01,\n          -1.1246e-01, -3.2821e-01,  1.8596e+00, -8.4703e-02, -4.5925e-01,\n           1.3872e+00]],\n\n        [[ 9.3675e-01, -7.2473e-01, -5.9262e-02, -1.5379e+00, -1.1964e+00,\n          -6.4501e-01,  1.5759e-01, -2.1482e-02,  1.0373e+00,  1.0561e+00,\n           6.1421e-02,  5.0943e-01, -3.8396e-02, -2.0337e-01,  2.6209e-01,\n          -1.1906e+00]],\n\n        [[-1.7010e-02, -1.3287e-01,  6.4031e-01,  4.3548e-01,  3.9059e+00,\n           2.5035e+00,  3.0248e-02,  2.4222e-01,  2.5349e+00,  1.1620e+00,\n           6.7421e-02, -4.6786e-01,  6.6147e-03, -3.3602e-01,  1.5848e+00,\n           7.4089e-01]],\n\n        [[ 3.5328e-01, -6.6715e-01, -1.7554e+00, -5.2718e-01,  4.4770e-01,\n           2.0356e-01,  6.5371e-01,  4.2300e-01, -1.7370e+00, -2.8210e-01,\n          -8.2915e-01,  1.1644e+00, -2.1914e-02, -1.2230e+00, -1.4447e+00,\n           5.2555e-01]]], device='cuda:0', grad_fn=<ViewBackward>)\nLSTM_out\ntensor([[ 9.8847e-02,  1.1711e-01, -3.4345e-02,  7.6953e-02,  1.0508e-01,\n         -1.6951e-01,  4.8078e-02, -1.4300e-01],\n        [ 6.7731e-02, -3.3515e-02, -4.4563e-02,  7.8696e-02, -3.4603e-02,\n          2.7593e-02,  1.2138e-01, -2.4505e-01],\n        [-2.7654e-02, -1.5292e-01,  1.4557e-02,  5.2039e-01, -2.7916e-01,\n          9.5345e-02, -8.0099e-03,  9.7362e-02],\n        [ 6.9657e-03,  4.9829e-02,  9.6436e-02,  3.5158e-01, -8.0728e-02,\n          4.5972e-02, -2.6094e-02, -2.5419e-01],\n        [ 1.4392e-01,  1.3664e-01, -1.3097e-01,  5.2642e-01, -9.3913e-02,\n         -7.2179e-02,  3.1211e-02, -3.2833e-01],\n        [-3.3743e-02,  1.4992e-01, -1.7527e-01,  4.1260e-01, -2.4468e-01,\n          2.9634e-01,  2.1887e-01, -2.9438e-01],\n        [-1.5634e-02,  6.1607e-02, -2.1915e-01, -6.9056e-04, -9.5239e-02,\n          1.0327e-01,  8.5867e-02, -1.4588e-01],\n        [ 8.0361e-02,  4.3044e-01, -8.4638e-02,  2.4482e-02, -6.0160e-02,\n         -1.4218e-01,  2.8465e-01, -2.0018e-01],\n        [ 3.9485e-03,  3.2425e-01, -3.0417e-01,  4.4406e-01, -5.7400e-02,\n         -1.9424e-01,  2.3334e-01, -1.2796e-01],\n        [-4.2013e-03,  1.3788e-01, -2.5178e-01,  4.3552e-01, -2.9462e-02,\n         -1.7901e-03, -3.1640e-02, -9.4551e-02],\n        [-3.6713e-04,  1.3898e-01, -1.1172e-01, -8.4733e-02, -1.6631e-01,\n         -7.0529e-02,  3.2007e-02, -1.0373e-01],\n        [ 3.7839e-02,  5.3491e-01, -8.8781e-02, -1.2348e-01, -3.9418e-02,\n         -8.8105e-02,  1.5098e-01, -2.6152e-01],\n        [ 1.0070e-01,  3.2487e-01, -1.9049e-01,  1.0694e-01,  1.1425e-02,\n         -5.8020e-02,  6.9285e-02, -2.2601e-01],\n        [ 8.4305e-02,  4.4079e-01, -3.3112e-01, -3.3406e-02,  7.3840e-03,\n          6.9241e-02,  4.5181e-02, -1.5801e-01],\n        [ 1.4500e-02,  1.3254e-01, -2.2191e-01, -1.9579e-01, -4.3483e-02,\n         -6.5546e-02,  7.7412e-02, -1.0270e-01],\n        [ 7.7233e-03, -1.4210e-01, -1.5041e-01, -2.9559e-02, -1.4882e-02,\n         -1.9932e-01,  2.9741e-01, -5.6772e-02],\n        [-5.2028e-02, -9.4592e-02, -2.1239e-01,  2.0717e-01, -2.5877e-01,\n         -3.7272e-01,  2.3411e-02, -2.8103e-01],\n        [ 4.8841e-02, -1.6703e-01, -7.8034e-02, -6.6361e-02, -8.1322e-02,\n         -3.4690e-01,  1.5953e-01, -2.0685e-01],\n        [-2.3768e-01, -1.1166e-01, -2.2485e-01, -1.8153e-01, -2.7037e-01,\n         -4.9355e-01,  6.0530e-02,  2.8446e-02],\n        [-5.8667e-02,  2.8425e-02, -2.8340e-01, -1.3777e-01, -2.1314e-01,\n         -4.2359e-02,  7.8741e-02, -3.7399e-02],\n        [ 4.0791e-02, -1.6713e-01, -1.8396e-01, -1.2872e-02,  4.8567e-03,\n          2.3412e-02,  3.8183e-02, -3.3753e-01],\n        [ 1.6887e-02, -4.8623e-02, -1.7555e-01,  1.7708e-01, -7.3577e-02,\n          4.6860e-03,  2.6216e-02, -4.7779e-01],\n        [-7.6855e-02,  7.9803e-03, -2.3253e-01, -1.6139e-01,  1.5266e-01,\n         -1.0757e-01,  2.5228e-01, -1.3437e-01],\n        [-1.9295e-01,  9.9108e-02, -1.8449e-01, -1.3203e-01,  9.0194e-02,\n          3.1305e-01,  2.8829e-01,  9.0464e-02],\n        [-7.9823e-03,  2.6723e-01, -9.8384e-02,  1.4731e-01, -7.1606e-02,\n          4.6188e-02,  1.7036e-01, -2.1524e-01],\n        [ 3.8148e-02, -4.8452e-02, -1.5324e-01,  6.1140e-02, -1.2705e-01,\n          1.3382e-01,  2.6917e-02, -1.0804e-01],\n        [-2.0175e-01,  6.7567e-02, -5.7612e-02, -4.7336e-02, -1.4404e-01,\n          2.4872e-01, -5.4320e-02, -1.7767e-01],\n        [-2.9742e-02, -9.8719e-02, -1.0121e-01, -2.1157e-01, -1.5064e-01,\n          1.8747e-01,  1.9136e-02, -8.9737e-02],\n        [-3.0782e-02, -3.5797e-01,  1.7409e-02,  3.1152e-01, -3.4781e-01,\n         -9.5346e-02, -5.0324e-02, -5.4796e-02],\n        [-2.3502e-02, -4.4984e-01, -3.4941e-02, -7.1748e-02, -1.0464e-01,\n         -1.6279e-01,  1.7557e-03, -1.1200e-01],\n        [ 1.3337e-01, -5.8081e-02,  8.5651e-02,  4.8636e-02, -6.6761e-02,\n         -1.9734e-04,  6.3073e-02,  2.9711e-01],\n        [ 1.7452e-02, -2.4434e-01, -1.9031e-03, -1.4100e-01, -2.6525e-02,\n         -1.5761e-01,  1.0601e-01,  1.8313e-02],\n        [-1.2465e-01,  6.3535e-02, -1.8252e-01, -1.4169e-01, -3.9730e-02,\n          2.9072e-01,  2.1261e-01,  1.7472e-01],\n        [-6.8231e-02, -6.0763e-02, -1.5653e-01,  1.2098e-01,  3.4143e-01,\n          3.7168e-01,  3.1307e-01,  2.0277e-01],\n        [-1.4691e-01,  6.9955e-03, -1.9567e-01, -2.6507e-01,  2.1882e-01,\n          4.9112e-02,  4.8157e-01,  1.5223e-01],\n        [-1.0874e-01, -8.2381e-02, -1.3832e-01,  4.9800e-02,  4.8515e-01,\n          2.9084e-01,  3.4806e-01,  2.4478e-01],\n        [-1.6645e-01,  8.9513e-02, -2.4179e-01, -2.9226e-02,  3.9793e-02,\n         -1.2713e-01,  2.5709e-01,  8.0445e-02],\n        [-7.8070e-02, -2.6329e-01, -5.8963e-02,  7.8926e-03, -4.7614e-03,\n          1.3447e-01,  2.7797e-01,  2.0526e-01],\n        [ 9.3034e-02, -2.5711e-01, -7.2287e-02, -2.7133e-01, -1.4809e-01,\n          3.8494e-01,  1.3092e-01, -7.1067e-02],\n        [-4.4651e-02, -2.9395e-01, -1.8397e-02,  2.2159e-01, -2.7621e-01,\n          2.0275e-01,  9.4211e-02,  1.3828e-01],\n        [ 6.1855e-02, -2.5959e-01, -3.8227e-02, -8.2989e-02, -8.4129e-02,\n         -1.1748e-02,  2.1561e-01, -1.7046e-02],\n        [ 7.6018e-02, -7.5751e-02, -1.8316e-01,  1.8988e-01, -4.4673e-02,\n         -3.5646e-02,  2.2110e-01, -1.9648e-01],\n        [ 7.5405e-02,  2.5637e-02, -1.9764e-01,  1.2659e-01, -7.7431e-02,\n         -5.4990e-02,  1.7733e-01, -1.4249e-01],\n        [ 1.9788e-02,  1.3846e-01, -4.2775e-02,  2.6443e-01, -8.2163e-02,\n         -5.8457e-02,  1.0657e-01, -3.6437e-01],\n        [ 2.7940e-01, -6.3877e-02, -1.4221e-01,  1.5808e-01, -1.9599e-02,\n          1.1085e-01,  7.8149e-02, -2.3099e-01],\n        [ 5.1325e-02,  1.0453e-01, -1.9193e-01,  1.0129e-01, -3.9892e-02,\n          5.0583e-01,  2.5168e-01, -1.2614e-01],\n        [-1.9707e-03, -3.6042e-02, -2.3663e-01, -1.2669e-01,  4.7147e-03,\n          2.2560e-01,  1.5708e-01, -9.6432e-02],\n        [-3.3932e-01,  6.7225e-02, -2.1098e-01,  2.1052e-01, -2.7832e-01,\n         -2.9548e-01, -5.6980e-02,  1.3421e-01],\n        [ 9.6112e-02,  1.3731e-01, -1.1645e-01,  2.1068e-01, -8.7032e-02,\n         -2.0205e-01,  2.5868e-02, -2.5422e-02]], device='cuda:0',\n       grad_fn=<ViewBackward>)\nPreact out\ntensor([[-0.2686, -0.3492, -0.2315,  ..., -0.0104,  0.0365,  0.1254],\n        [-0.3553, -0.3133, -0.2564,  ...,  0.0531,  0.1652, -0.0357],\n        [-0.5477, -0.1864, -0.1897,  ...,  0.1419,  0.2563, -0.1419],\n        ...,\n        [-0.4449, -0.3013, -0.2928,  ...,  0.0770,  0.0916, -0.0587],\n        [-0.3370, -0.3996, -0.2479,  ...,  0.0095,  0.1242,  0.0675],\n        [-0.3280, -0.3113, -0.1781,  ...,  0.0638,  0.0331,  0.1049]],\n       device='cuda:0', grad_fn=<ViewBackward>)\nEpoch:            0 Loss: 7.8099684715271\n"
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for x, y in zip(X_train, Y_train):\n",
    "        # Limpiamos gradientes acumulados\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Forward\n",
    "        # print(x)\n",
    "        pred = model(x)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        loss = criterion(pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch: {epoch:>12} Loss: {loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37764bitaypit20202conda2eb9642ad5e346f38de29b14c14576bc",
   "display_name": "Python 3.7.7 64-bit ('aypit-2020-2': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}